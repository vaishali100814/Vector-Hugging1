{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Liabraries Assignment"
      ],
      "metadata": {
        "id": "caewoJgf7lwl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: Compare and contrast NLTK and spaCy in terms of features, ease of use,\n",
        "and performance.\n",
        "  - NLTK (Natural Language Toolkit) and spaCy are both prominent Python libraries for Natural Language Processing (NLP), but they differ significantly in their approach and target audience.\n",
        "  Features:\n",
        "  NLTK: Offers a wide array of algorithms and resources, including tokenizers, stemmers, taggers, parsers, and various corpora. It provides extensive flexibility for research and experimentation, allowing users to choose and compare different algorithms for the same task. However, it lacks built-in support for word embeddings and advanced neural network models.\n",
        "  spaCy: Focuses on providing efficient and production-ready NLP pipelines with pre-trained statistical models for various languages. It includes features like tokenization, part-of-speech tagging, named entity recognition, dependency parsing, and word vectors. spaCy is designed for speed and accuracy in real-world applications.\n",
        "  Ease of use:\n",
        "  NLTK: Can have a steeper learning curve due to its extensive collection of algorithms and the need for manual configuration for many tasks. It's more geared towards researchers who want fine-grained control over the NLP process.\n",
        "  spaCy: Is generally considered more user-friendly, especially for developers and those looking for quick integration into applications. It offers a streamlined API and pre-trained models that simplify common NLP tasks, making it easier to get started and achieve results.\n",
        "  Perfomance:\n",
        "  NLTK: Performance can vary depending on the chosen algorithms and the complexity of the task. While it offers a wide range of options, optimizing for speed and efficiency often requires careful selection and configuration.\n",
        "  spaCy: Is optimized for performance and speed, making it suitable for large-scale text processing and real-time applications. Its pre-trained models and efficient algorithms generally deliver faster processing times and higher accuracy for many standard NLP tasks compared to NLTK's default settings.\n",
        "\n",
        "Question 2: What is TextBlob and how does it simplify common NLP tasks like\n",
        "sentiment analysis and translation?\n",
        "  - TextBlob is a Python library that simplifies common Natural Language Processing (NLP) tasks by providing a simple, intuitive, high-level API built on top of the more complex NLTK and Pattern libraries. It allows beginners and professionals to perform operations like sentiment analysis and translation with minimal code.\n",
        "  TextBlob streamlines sentiment analysis by abstracting the complex underlying rule-based models into a single, accessible property of a TextBlob object.\n",
        "  Simple API: You simply create a TextBlob object from your text and access its .sentiment property.\n",
        "  Direct Scores: This property returns a tuple containing two floats: polarity and subjectivity.\n",
        "  Interpretable Results: The polarity score ranges from -1.0 (very negative) to 1.0 (very positive), while subjectivity ranges from 0.0 (objective) to 1.0 (subjective), making the results immediately understandable without needing to manage the underlying dictionaries or models.\n",
        "\n",
        "Question 3: Explain the role of Standford NLP in academic and industry NLP Projects.\n",
        "  - Stanford NLP serves as a foundational suite of NLP tools and libraries, significantly influencing both academic research and industry applications. [1] It provides a comprehensive collection of pre-trained models and robust software for common tasks like tokenization, part-of-speech tagging, and named entity recognition.\n",
        "  In academia, Stanford NLP is heavily utilized as a baseline for new research and a core teaching tool.\n",
        "  Benchmarking: Researchers often use its models as a standard to compare against novel algorithms and models developed in their studies. [2]\n",
        "  Education: It is frequently integrated into university curricula to teach students the fundamentals of how NLP tasks are implemented.\n",
        "  Accessible Research: The open-source nature and well-documented APIs allow researchers globally to build upon existing work efficiently.\n",
        "\n",
        "Question 4: Describe the architecture and functioning of a Recurrent Natural Network\n",
        "(RNN).\n",
        "  - A Recurrent Neural Network (RNN) is a class of artificial neural networks designed to recognize patterns in sequences of data, such as text, handwriting, or spoken language. Unlike traditional feedforward networks where information flows in only one direction, RNNs incorporate loops that allow information to persist, effectively giving them a form of memory about previous inputs. This architecture makes them highly suitable for tasks involving sequential data.\n",
        "  Architecture of an RNN:\n",
        "  The Recurrent Structure\n",
        "  Input Layer: Receives the data for the current time step (e.g., a single word in a sentence).\n",
        "  Hidden State (Memory): This is the crucial component. At each time step, the network takes the current input and the previous hidden state (the output from the loop in the prior step) to calculate a new hidden state. This hidden state captures information learned from all preceding elements in the sequence.\n",
        "  Output Layer: Uses the current hidden state to produce an output (e.g., the next predicted word, a sentiment score).\n",
        "\n",
        "Question 5: What is the key difference between LSTM and GRU networks in NLP\n",
        "applications?\n",
        "  - The key difference between LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit) networks is the number and type of gates they use to control information flow. LSTMs employ three gates: the input gate, forget gate, and output gate, alongside a dedicated cell state. GRUs, designed for simplicity, use only two gates: the reset gate and the update gate, and lack a separate cell state, merging the cell state and hidden state into one.\n",
        "  The architectural disparity leads to LSTMs having more parameters and potentially higher accuracy in specific complex sequence modeling tasks, while GRUs are generally faster to train and computationally less intensive due to their simpler structure.\n",
        "  Key components:\n",
        "  LSTM: Utilizes a cell state to carry information across time steps, governed by the three distinct gates that regulate what information to remember, forget, and output. This provides a more granular control over information flow.\n",
        "  GRU: Merges the cell state and hidden state, simplifying the architecture. The two gates control how much of the past information to forget (reset gate) and how much of the new information to incorporate into the current state (update gate).\n",
        "  \n",
        "   "
      ],
      "metadata": {
        "id": "iPB5GKdu7rV0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwYl1edp7Zha",
        "outputId": "e22c71c6-0897-496a-9a24-cb21fc4ed117"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment(polarity=0.0, subjectivity=0.7)\n"
          ]
        }
      ],
      "source": [
        "# Write a Python program using TextBlob to perform sentiment analysis on the following paragraph:\n",
        "\n",
        "text = \"I enjoy coding, but debugging can be frustrating.\"\n",
        "blob = TextBlob(text)\n",
        "sentiment = blob.sentiment\n",
        "print(sentiment)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Given the sample paragraph below, perform string tokenization and frequency distribution using Python and NLTK:\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "# Download necessary NLTK data (if not already downloaded)\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except nltk.downloader.DownloadError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "# Sample paragraph\n",
        "paragraph = \"Natural Language Processing (NLP) is a fascinating field that combines linguistics, computer science, and artificial intelligence. It enables machines to understand, interpret, and generate human language. Applications of NLP include chatbots, sentiment analysis, and machine translation. As technology advances, the role of NLP in modern solutions is becoming increasingly critical.\"\n",
        "\n",
        "# 1. Tokenization\n",
        "# Convert the paragraph to lowercase to treat \"NLP\" and \"nlp\" as the same token\n",
        "lower_case_paragraph = paragraph.lower()\n",
        "tokens = word_tokenize(lower_case_paragraph)\n",
        "\n",
        "print(\"Tokens:\")\n",
        "print(tokens)\n",
        "\n",
        "# 2. Frequency Distribution\n",
        "fdist = FreqDist(tokens)\n",
        "\n",
        "print(\"\\nFrequency Distribution:\")\n",
        "for word, frequency in fdist.most_common(10):  # Display top 10 most common words\n",
        "    print(f\"{word}: {frequency}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        },
        "id": "RAQT3GuC-vz4",
        "outputId": "e588bef8-e04e-46e5-a590-78ff67e63b12"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'nltk.downloader' has no attribute 'DownloadError'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-741616736.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDownloadError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-741616736.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mexcept\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDownloadError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'punkt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'nltk.downloader' has no attribute 'DownloadError'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Question 8. Implement a basic LSTM model in Keras for a text classification task using the following dummy dataset.Your model should classify sentences.\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# 1. Dummy Dataset (already provided in the prompt)\n",
        "texts = [\n",
        "    \"I love this project\",\n",
        "    \"This is an amazing experience\",\n",
        "    \"I hate waiting in line\",\n",
        "    \"This is the worst service\",\n",
        "    \"Absolutely fantastic!\"\n",
        "]\n",
        "labels = [1, 1, 0, 0, 1]\n",
        "\n",
        "# Convert labels to a numpy array\n",
        "labels = np.array(labels)\n",
        "\n",
        "# 2. Text Preprocessing and Tokenization\n",
        "vocab_size = 100\n",
        "oov_tok = \"<OOV>\" # Out of vocabulary token\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "print(f\"Word Index: {word_index}\\n\")\n",
        "\n",
        "# Convert texts to sequences of integers\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "print(f\"Sequences: {sequences}\\n\")\n",
        "\n",
        "# 3. Padding Sequences\n",
        "max_length = max([len(x) for x in sequences]) # Determine max length in the dataset\n",
        "padding_type='post'\n",
        "trunc_type='post'\n",
        "\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "print(f\"Padded Sequences (Shape: {padded_sequences.shape}):\\n{padded_sequences}\\n\")\n",
        "\n",
        "# 4. Building the LSTM Model\n",
        "embedding_dim = 16\n",
        "\n",
        "model = Sequential([\n",
        "    # Input layer expects integer indices, outputs dense vectors of size embedding_dim\n",
        "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length),\n",
        "    # LSTM layer processes the sequence data\n",
        "    LSTM(units=32),\n",
        "    # Dense output layer with a single neuron and sigmoid activation for binary classification\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# 5. Training the Model\n",
        "num_epochs = 50\n",
        "\n",
        "# Note: With such a tiny dataset, the results are unlikely to generalize well or show smooth learning curves,\n",
        "# but it demonstrates the implementation process.\n",
        "history = model.fit(\n",
        "    padded_sequences,\n",
        "    labels,\n",
        "    epochs=num_epochs,\n",
        "    verbose=0 # Set verbose=1 to see training progress per epoch\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining finished after {num_epochs} epochs.\")\n",
        "\n",
        "# Optional: Display final training accuracy\n",
        "loss, accuracy = model.evaluate(padded_sequences, labels, verbose=0)\n",
        "print(f\"Final Training Accuracy: {accuracy*100:.2f}%\")\n",
        "\n",
        "# 6. Example Prediction Function\n",
        "def predict_sentiment(text_list):\n",
        "    # Preprocess new text exactly like the training data\n",
        "    new_sequences = tokenizer.texts_to_sequences(text_list)\n",
        "    new_padded = pad_sequences(new_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "    # Predict\n",
        "    predictions = model.predict(new_padded)\n",
        "\n",
        "    for text, pred in zip(text_list, predictions):\n",
        "        sentiment = \"Positive (1)\" if pred[0] >= 0.5 else \"Negative (0)\"\n",
        "        print(f\"Text: '{text}' -> Prediction Score: {pred[0]:.4f} -> Sentiment: {sentiment}\")\n",
        "\n",
        "print(\"\\n--- Testing Model Predictions ---\")\n",
        "test_texts = [\n",
        "    \"I love this!\", # Should lean positive\n",
        "    \"Worst experience ever\", # Should lean negative\n",
        "    \"It was okay\" # Neutral/Unseen words\n",
        "]\n",
        "\n",
        "predict_sentiment(test_texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 640
        },
        "id": "t1iob_mH_e5L",
        "outputId": "48e3f0a4-edab-420a-cfa6-b36c6ef4dde4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Index: {'<OOV>': 1, 'this': 2, 'i': 3, 'is': 4, 'love': 5, 'project': 6, 'an': 7, 'amazing': 8, 'experience': 9, 'hate': 10, 'waiting': 11, 'in': 12, 'line': 13, 'the': 14, 'worst': 15, 'service': 16, 'absolutely': 17, 'fantastic': 18}\n",
            "\n",
            "Sequences: [[3, 5, 2, 6], [2, 4, 7, 8, 9], [3, 10, 11, 12, 13], [2, 4, 14, 15, 16], [17, 18]]\n",
            "\n",
            "Padded Sequences (Shape: (5, 5)):\n",
            "[[ 3  5  2  6  0]\n",
            " [ 2  4  7  8  9]\n",
            " [ 3 10 11 12 13]\n",
            " [ 2  4 14 15 16]\n",
            " [17 18  0  0  0]]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training finished after 50 epochs.\n",
            "Final Training Accuracy: 80.00%\n",
            "\n",
            "--- Testing Model Predictions ---\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step\n",
            "Text: 'I love this!' -> Prediction Score: 0.8529 -> Sentiment: Positive (1)\n",
            "Text: 'Worst experience ever' -> Prediction Score: 0.7783 -> Sentiment: Positive (1)\n",
            "Text: 'It was okay' -> Prediction Score: 0.8193 -> Sentiment: Positive (1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 9. Using spaCy, build a simple NLP pipeline that includes tokenization,lemmatization, and entity recognition.\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Load the English NLP model\n",
        "# You might need to run `python -m spacy download en_core_web_sm` in your terminal first\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# The provided text dataset\n",
        "text = \"Homi Jehangir Bhaba was an Indian nuclear physicist who played a key role in the development of India's atomic energy program. He was the founding director of the Tata Institute of Fundamental Research (TIFR) and was instrumental in establishing the Atomic Energy Commission of India.\"\n",
        "\n",
        "# Process the text with the spaCy pipeline\n",
        "doc = nlp(text)\n",
        "\n",
        "# --- 1. Tokenization and Lemmatization ---\n",
        "print(\"--- Token, Lemma, POS Tag ---\")\n",
        "for token in doc:\n",
        "    # Print the token text, its lemma, and its part-of-speech tag for clarity\n",
        "    print(f\"{token.text:<20} {token.lemma_:<20} {token.pos_:<10}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*40 + \"\\n\")\n",
        "\n",
        "# --- 2. Named Entity Recognition (NER) ---\n",
        "print(\"--- Named Entities ---\")\n",
        "for ent in doc.ents:\n",
        "    # Print the entity text and its label (type)\n",
        "    print(f\"{ent.text:<40} {ent.label_:<10}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8rW5QW9_5tC",
        "outputId": "046c047e-92cc-4220-a840-e490c2274394"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Token, Lemma, POS Tag ---\n",
            "Homi                 Homi                 PROPN     \n",
            "Jehangir             Jehangir             PROPN     \n",
            "Bhaba                Bhaba                PROPN     \n",
            "was                  be                   AUX       \n",
            "an                   an                   DET       \n",
            "Indian               indian               ADJ       \n",
            "nuclear              nuclear              ADJ       \n",
            "physicist            physicist            NOUN      \n",
            "who                  who                  PRON      \n",
            "played               play                 VERB      \n",
            "a                    a                    DET       \n",
            "key                  key                  ADJ       \n",
            "role                 role                 NOUN      \n",
            "in                   in                   ADP       \n",
            "the                  the                  DET       \n",
            "development          development          NOUN      \n",
            "of                   of                   ADP       \n",
            "India                India                PROPN     \n",
            "'s                   's                   PART      \n",
            "atomic               atomic               ADJ       \n",
            "energy               energy               NOUN      \n",
            "program              program              NOUN      \n",
            ".                    .                    PUNCT     \n",
            "He                   he                   PRON      \n",
            "was                  be                   AUX       \n",
            "the                  the                  DET       \n",
            "founding             found                VERB      \n",
            "director             director             NOUN      \n",
            "of                   of                   ADP       \n",
            "the                  the                  DET       \n",
            "Tata                 Tata                 PROPN     \n",
            "Institute            Institute            PROPN     \n",
            "of                   of                   ADP       \n",
            "Fundamental          Fundamental          PROPN     \n",
            "Research             Research             PROPN     \n",
            "(                    (                    PUNCT     \n",
            "TIFR                 TIFR                 PROPN     \n",
            ")                    )                    PUNCT     \n",
            "and                  and                  CCONJ     \n",
            "was                  be                   AUX       \n",
            "instrumental         instrumental         ADJ       \n",
            "in                   in                   ADP       \n",
            "establishing         establish            VERB      \n",
            "the                  the                  DET       \n",
            "Atomic               Atomic               PROPN     \n",
            "Energy               Energy               PROPN     \n",
            "Commission           Commission           PROPN     \n",
            "of                   of                   ADP       \n",
            "India                India                PROPN     \n",
            ".                    .                    PUNCT     \n",
            "\n",
            "========================================\n",
            "\n",
            "--- Named Entities ---\n",
            "Homi Jehangir Bhaba                      FAC       \n",
            "Indian                                   NORP      \n",
            "India                                    GPE       \n",
            "the Tata Institute of Fundamental Research ORG       \n",
            "the Atomic Energy Commission of India    ORG       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working on a chatbot for a mental health platform. Explain how\n",
        "you would leverage LSTM or GRU networks along with libraries like spaCy or Stanford\n",
        "NLP to understand and respond to user input effectively. Detail your architecture, data\n",
        "preprocessing pipeline, and any ethical considerations.\n",
        "(Include your Python code and output in the code box below.\n",
        "  - For starters, mental health disorders and problems affect an estimated 792 million people worldwide.\n",
        "  That’s basically 1 in 10 people globally.\n",
        "  In Canada, where I’m from, the problem is even worse, as 1 in 5 Canadians experience a mental illness or addiction problem every single year, with 1 in 2 experiencing one by the time that they reach the age of 40.\n",
        "  70% of the mental health problems also begun during childhood or adolescence, and youth experiencing the highest rates than any other age group. The reason that this is such a big problem is that mental illness can reduce life expectancy by 10–20 years.\n",
        "  "
      ],
      "metadata": {
        "id": "_zqAInbgA23m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer 10.\n",
        "\n",
        "import numpy as np\n",
        "import nltk\n",
        "# nltk.download('punkt')\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "def tokenize(sentence):\n",
        "    \"\"\"\n",
        "    split sentence into array of words/tokens\n",
        "    a token can be a word or punctuation character, or number\n",
        "    \"\"\"\n",
        "    return nltk.word_tokenize(sentence)\n",
        "def stem(word):\n",
        "    \"\"\"\n",
        "    stemming = find the root form of the word\n",
        "    examples:\n",
        "    words = [\"organize\", \"organizes\", \"organizing\"]\n",
        "    words = [stem(w) for w in words]\n",
        "    -> [\"organ\", \"organ\", \"organ\"]\n",
        "    \"\"\"\n",
        "    return stemmer.stem(word.lower())\n",
        "def bag_of_words(tokenized_sentence, words):\n",
        "    \"\"\"\n",
        "    return bag of words array:\n",
        "    1 for each known word that exists in the sentence, 0 otherwise\n",
        "    example:\n",
        "    sentence = [\"hello\", \"how\", \"are\", \"you\"]\n",
        "    words = [\"hi\", \"hello\", \"I\", \"you\", \"bye\", \"thank\", \"cool\"]\n",
        "    bog   = [  0 ,    1 ,    0 ,   1 ,    0 ,    0 ,      0]\n",
        "    \"\"\"\n",
        "    # stem each word\n",
        "    sentence_words = [stem(word) for word in tokenized_sentence]\n",
        "    # initialize bag with 0 for each word\n",
        "    bag = np.zeros(len(words), dtype=np.float32)\n",
        "    for idx, w in enumerate(words):\n",
        "        if w in sentence_words:\n",
        "            bag[idx] = 1\n",
        "    return bag"
      ],
      "metadata": {
        "id": "gTP4zhc0Arg1"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}